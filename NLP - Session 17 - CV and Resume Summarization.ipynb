{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Session 17 - CV and Resume Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume NER Training\n",
    "In this blog, we are going to create a model using SpaCy which will extract the main points from a resume. We are going to train the model on almost 200 resumes. After the model is ready, we will extract the text from a new resume and pass it to the model to get the summary.\n",
    "\n",
    "Collecting training data is a very crucial step while building any machine learning model. It may sound like an incredibly painful process. In this project, we have used about 200 resumes to train our model.\n",
    "\n",
    "You can download the dataset from \n",
    "\n",
    "https://github.com/laxmimerit/Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python.\n",
    "\n",
    "Follow Example Here: https://spacy.io/usage/training#training-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the training data. The data consists of the contents of the resume which is extracted from a PDF file, followed by a dictionary consisting of a label and the start and end index of the value in the resume. In the example given below `Companies worked at` is a custom label and there are multiple values for it in the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open(\"data/cv-resume.pkl\", \"rb\"))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load a black SpaCy english model. Then we will write a function which will take the training data as the input. In the function, first we will add a ner i.e. Named Entity Recognition in the last position in the pipeline. Then we will add our custom labels in the pipeline.\n",
    "\n",
    "Now we are going to prepare our data for training. We are disable all the pipeline components except ner. We are only going to train ner. We are going to train for 10 iterations. At each iteration, the training data is shuffled to ensure the model doesn\u2019t make any generalizations based on the order of examples. We are again going to read the training data. Another technique to improve the learning results is to set a dropout rate, a rate at which to randomly \u201cdrop\u201d individual features and representations. This makes it harder for the model to memorize the training data. We have added a droupout of 0.2 which means that each feature or internal representation has a 1/5 likelihood of being dropped.\n",
    "\n",
    "Lastly, we will train the model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "def train_model(train_data):\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "\n",
    "    for _, annotation in train_data:\n",
    "        for ent in annotation[\"entities\"]:\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            index = 0\n",
    "            for text, annotations in train_data:\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        [text],  # batch of texts\n",
    "                        [annotations],  # batch of annotations\n",
    "                        drop=0.2,  # dropout - make it harder to memorise data\n",
    "                        sgd=optimizer,  # callable to update weights\n",
    "                        losses=losses,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will take a lot of time to train. So we are saving the model for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"nlp_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the saved model into nlp_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = spacy.load(\"nlp_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first resume from our training data. Due to `random.shuffle(train_data)` used in the function train_model() we are getting a different resume at the first position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pass this resume to our model and see the results. We have used some formatting while printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_model(train_data[0][0])\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.label_.upper():{30}}- {ent.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our model on an unseen resume. As the resume is in the PDF format we will extract the text from the PDF file using PyMuPDF. Then we will pass the text to our model and see the results.\n",
    "\n",
    "You can install PyMuPDF using the following command:-\n",
    "\n",
    "`conda activate tensorflow20`\n",
    "\n",
    "`!pip install PyMuPDF`\n",
    "\n",
    "We are opening the `Alice Clark CV.pdf`. Then we are extracting the text using `getText()`. After that we are removing the new line characters `'\\n'` from the text.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import fitz\n",
    "\n",
    "fname = \"data/Alice Clark CV.pdf\"\n",
    "doc = fitz.open(fname)\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text = text + str(page.getText())\n",
    "\n",
    "tx = \" \".join(text.split(\"\\n\"))\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pass the extracted text to our model and get the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_model(tx)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.label_.upper():{30}}- {ent.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better and accurate summary you can train the model on more data samples. You can include different kinds of resumes in the training samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
