{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Session 6 - SpaCy Introduction for NLP | Linguistic Annotaion | Emoji, Phone, Email, HashTag Extraction and Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linguistic Annotations\n",
    "Let\u2019s say you\u2019re analyzing user comments and you want to find out what people are saying about Facebook. You want to start off by finding adjectives following \u201cFacebook is\u201d or \u201cFacebook was\u201d. This is obviously a very rudimentary solution, but it\u2019ll be fast, and a great way to get an idea for what\u2019s in your data. Your pattern could look like this:\n",
    "\n",
    "`[{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]`\n",
    "\n",
    "This translates to a token whose lowercase form matches \u201cfacebook\u201d (like Facebook, facebook or FACEBOOK), followed by a token with the lemma \u201cbe\u201d (for example, is, was, or \u2018s), followed by an optional adverb, followed by an adjective.\n",
    "\n",
    "This is the link for all the annotations-\n",
    "\n",
    "https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LOWER\": \"facebook\"},\n",
    "    {\"LEMMA\": \"be\"},\n",
    "    {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "    {\"POS\": \"ADJ\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_method_fb(matcher, doc, i, matches):\n",
    "    matched_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    sent = span.sent\n",
    "\n",
    "    match_ents = [\n",
    "        {\n",
    "            \"start\": span.start_char - sent.start_char,\n",
    "            \"end\": span.end_char - sent.start_char,\n",
    "            \"label\": \"MATCH\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"fb\", callback_method_fb, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I'd say that Facebook is evil. \u2013 Facebook is pretty cool, right?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phone Number\n",
    "Phone numbers can have many different formats and matching them is often tricky. During tokenization, spaCy will leave sequences of numbers intact and only split on whitespace and punctuation. This means that your match pattern will have to look out for number sequences of a certain length, surrounded by specific punctuation \u2013 depending on the national conventions.\n",
    "\n",
    "You want to match like this `(123) 4567 8901` or `(123) 4567-8901`\n",
    "\n",
    "`[{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"dddd\"}, {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]`\n",
    "\n",
    "In this pattern we are looking for a opening bracket. Then we are matching a number with 3 digits. Then a closing bracket. Then a number with 4 digits. Then a dash which is optional. Lastly, a number with 4 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"ORTH\": \"(\"},\n",
    "    {\"SHAPE\": \"ddd\"},\n",
    "    {\"ORTH\": \")\"},\n",
    "    {\"SHAPE\": \"dddd\"},\n",
    "    {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "    {\"SHAPE\": \"dddd\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PhoneNumber\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Call me at (123) 4560-7890\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Address Matching\n",
    "In this the pattern checks for one or more character from a-zA-Z0-9-_.. Then a @. Then again one or more character from a-zA-Z0-9-_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"REGEX\": \"[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"Email\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Email me at email2me@kgptalkie.com and talk.me@kgptalkie.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags and emoji on social media\n",
    "Social media posts, especially tweets, can be difficult to work with. They\u2019re very short and often contain various emoji and hashtags. By only looking at the plain text, you\u2019ll lose a lot of valuable semantic information.\n",
    "\n",
    "Let\u2019s say you\u2019ve extracted a large sample of social media posts on a specific topic, for example posts mentioning a brand name or product. As the first step of your data exploration, you want to filter out posts containing certain emoji and use them to assign a general sentiment score, based on whether the expressed emotion is positive or negative, e.g. \ud83d\ude00 or \ud83d\ude1e. You also want to find, merge and label hashtags like #MondayMotivation, to be able to ignore or analyze them later.\n",
    "\n",
    "By default, spaCy\u2019s tokenizer will split emoji into separate tokens. This means that you can create a pattern for one or more emoji tokens. Valid hashtags usually consist of a #, plus a sequence of ASCII characters with no whitespace, making them easy to match as well.\n",
    "\n",
    "We have made a list of positive and negative emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emoji = [\"\ud83d\ude00\", \"\ud83d\ude03\", \"\ud83d\ude02\", \"\ud83e\udd23\", \"\ud83d\ude0a\", \"\ud83d\ude0d\"]  # Positive emoji\n",
    "neg_emoji = [\"\ud83d\ude1e\", \"\ud83d\ude20\", \"\ud83d\ude29\", \"\ud83d\ude22\", \"\ud83d\ude2d\", \"\ud83d\ude12\"]  # Negative emoji\n",
    "pos_emoji, neg_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add patterns to match one or more emoji tokens\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "pos_patterns, neg_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a function label_sentiment() which will be called after every match to label the sentiment of the emoji. If the sentiment is positive then we are adding 0.1 to doc.sentiment and if the sentiment is negative then we are subtracting 0.1 from doc.sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":\n",
    "        doc.sentiment += 0.1\n",
    "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
    "        doc.sentiment -= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"HAPPY\", label_sentiment, *pos_patterns)\n",
    "matcher.add(\"SAD\", label_sentiment, *neg_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here with the HAPPY and SAD matchers we are also adding HASHTAG matcher to extract the hashtags. For hashtags we are going to match text which has atleast one \u2018#\u2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"HASHTAG\", None, [{\"TEXT\": \"#\"}, {\"IS_ASCII\": True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello world \ud83d\ude00 #DATASCIENCE \ud83d\ude22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient phrase matching\n",
    "If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall. The Doc patterns can contain single or multiple tokens.\n",
    "\n",
    "We are going to extract the names in terms from a document. We have made a pattern for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"DONALD TRUMP\", \"ANGELA MERKEL\", \"WASHINGTON D.C.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [nlp.make_doc(text) for text in terms]\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"term\", None, *pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"German Chancellor ANGELA MERKEL and US President DONALD TRUMP \"\n",
    "    \"converse in the Oval Office inside the White House in WASHINGTON D.C.\"\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Rule Based Entity Recognition\n",
    "The EntityRuler is an exciting new component that lets you add named entities based on pattern dictionaries, and makes it easy to combine rule-based and statistical named entity recognition for even more powerful models.\n",
    "\n",
    "### Entity Patterns\n",
    "Entity patterns are dictionaries with two keys: \u201clabel\u201d, specifying the label to assign to the entity if the pattern is matched, and \u201cpattern\u201d, the match pattern. The entity ruler accepts two types of patterns:\n",
    "\n",
    " - Phrase Pattern `{\"label\": \"ORG\", \"pattern\": \"Apple\"}`\n",
    " - Token Pattern `{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}`\n",
    "\n",
    "### Using the entity ruler\n",
    "The EntityRuler is a pipeline component that\u2019s typically added via `nlp.add_pipe`. When the nlp object is called on a text, it will find matches in the doc and add them as entities to the doc.ents, using the specified pattern label as the entity label.\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "We are importing EntityRuler from spacy.pipeline. Then we are loading a fresh model using `spacy.load()`. We have created a pattern which will label KGP Talkie as ORG and san francisco as GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = EntityRuler(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    {\"label\": \"ORG\", \"pattern\": \"KGP Talkie\"},\n",
    "    {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]},\n",
    "]\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"KGP Talkie is opening its first big office in San Francisco.\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to using only regular expressions on raw text, spaCy\u2019s rule-based matcher engines and components not only let you find the words and phrases you\u2019re looking for \u2013 they also give you access to the tokens within the document and their relationships. This means you can easily access and analyze the surrounding tokens, merge spans into single tokens or add entries to the named entities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
