{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP - Session 18 - Building Text Cleanup and PreProcessing Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNf8JbGHb8riXOHlhxXN4+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-nlp/blob/main/NLP%20-%20Session%2018%20-%20Building%20Text%20Cleanup%20and%20PreProcessing%20Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP - Session 18 - Building Text Cleanup and PreProcessing Pipeline**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cVInA9Pi0X2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing HTML Tags**"
      ],
      "metadata": {
        "id": "Ul5aUcqBUAx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags(text):\n",
        "  return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "remove_html_tags(\n",
        "    \"<html> \\\n",
        "      <h1>Article Heading</h1> \\\n",
        "      <p>First sentence of some important article. And another one. And then the last one</p></html>\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g8E1Tv5_UJ2Q",
        "outputId": "788c21f0-637c-4e80-cceb-5a0a71406721"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Article Heading First sentence of some important article. And another one. And then the last one'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Accented Characters**"
      ],
      "metadata": {
        "id": "Sfvj-Jo1WAGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_characters(text):\n",
        "  return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
        "   \n",
        "remove_accented_characters(\"Sómě Áccěntěd těxt. Some words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vrl41oS_WCnd",
        "outputId": "83da2aca-1cec-49ec-c4eb-25f04477c238"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text. Some words such as resume, cafe, protest, divorce, coordinate, expose, latte.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expanding Contractions**\n",
        "\n",
        "Contractions are shortened versions of words or syllables. They are created by removing specific, one or more letters from words. "
      ],
      "metadata": {
        "id": "gNzqo1DNX5g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "XsZcir2HZWBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from contractions import contractions_dict \n",
        "import re\n",
        "\n",
        "def expand_contractions(text, map=contractions_dict):\n",
        "  pattern = re.compile(\"({})\".format(\"|\".join(map.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
        "\n",
        "  def get_match(contraction):\n",
        "    match = contraction.group(0)\n",
        "    first_char = match[0]\n",
        "    expanded = map.get(match) \n",
        "    expanded = first_char + expanded[1:]\n",
        "    return expanded\n",
        "\n",
        "  new_text = pattern.sub(get_match, text)\n",
        "  new_text = re.sub(\"'\", new_text)\n",
        "  return new_text\n",
        "\n",
        "expand_contractions(\"Y’all i’d contractions you’re expanded don’t think.\")"
      ],
      "metadata": {
        "id": "Ys4S06TfYFPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Special Characters**"
      ],
      "metadata": {
        "id": "iuvc6WxrphvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "  pat = \"[^a-zA-Z0-9.,!?/:;\\\"\\'\\s]\"\n",
        "  return re.sub(pat, \"\", text)\n",
        "\n",
        "remove_special_characters(\"007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6I3xtNuKpmiW",
        "outputId": "dcabcdbe-55cd-459e-8e34-c32221e7d2c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'007 Not sure if this  was fun! 558923 What do you think of it.? 500USD!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Numbers**"
      ],
      "metadata": {
        "id": "E9qm5Uu1qSwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import re\n",
        "\n",
        "# function to remove numbers\n",
        "def remove_numbers(text):\n",
        "    # define the pattern to keep\n",
        "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
        "    return re.sub(pattern, '', text)\n",
        " \n",
        "# call function\n",
        "remove_numbers(\"007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tDOoUzCtqVTM",
        "outputId": "9e9b1716-73e8-4f79-a262-010adb21e103"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Not sure if this  was fun!  What do you think of it.? USD!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation**"
      ],
      "metadata": {
        "id": "ir0MN0yPqejX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  text = \"\".join([c for c in text if c not in string.punctuation])\n",
        "  return text\n",
        "\n",
        "remove_punctuation('Article: @First sentence of some, {important} article having lot of ~ punctuations. And another one;!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iQBzAQibqhwC",
        "outputId": "aa929480-468c-4543-a827-fcbffedefcb6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Article First sentence of some important article having lot of  punctuations And another one'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "ysbKMYb7rAkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def get_stem(text):\n",
        "  stemmer = nltk.porter.PorterStemmer()\n",
        "  text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "get_stem(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qI5a-JuzrCNr",
        "outputId": "11cc3dfa-f95a-40fc-b115-982ce6dd06de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we are eat and swim ; we have been eat and swim ; he eat and swim ; he ate and swam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "mn34G8SIrj6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\", parse=True, tag=True, entity=True)\n",
        "\n",
        "def get_lem(text):\n",
        "  text = nlp(text)\n",
        "  text = \" \".join([word.lemma_ if word.lemma_ != \"-PRON-\" else word.text for word in text])\n",
        "  return text\n",
        "\n",
        "get_lem(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6NBIu-VCrk_y",
        "outputId": "9d42c3a7-21bb-4536-b5db-c0651d1c4e1a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we be eat and swim ; we have be eat and swim ; he eat and swim ; he eat and swam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Stopwords**"
      ],
      "metadata": {
        "id": "P1D1ebLaseDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "import nltk\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  t = [token for token in tokens if token.lower() not in stopword_list]\n",
        "  text = \" \".join(t)\n",
        "  return text\n",
        "\n",
        "remove_stopwords(\"i am myself you the stopwords list and this article is not should removed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_3ySOObZsm6A",
        "outputId": "1195a792-9361-474b-9d9a-0bc7b3920045"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stopwords list article removed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing extra Whitespaces and Tabs**"
      ],
      "metadata": {
        "id": "KmvqLYGOucO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_extra_whitespace_tabs(text):\n",
        "  pattern = \"^\\s*|\\s\\s*\"\n",
        "  return re.sub(pattern, \" \", text).strip()\n",
        "\n",
        "remove_extra_whitespace_tabs('  This web line  has \\t some extra  \\t   tabs and whitespaces  ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I2D3b62kudXI",
        "outputId": "c9974c1c-1c4e-4ebf-d767-81dfa4c6dbba"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This web line has some extra tabs and whitespaces'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lowercase**"
      ],
      "metadata": {
        "id": "T2M6-HUTu9tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "to_lowercase('ConVert THIS string to LOWER cASe.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0SKzpfvCvAMI",
        "outputId": "41f68f06-7256-4032-9092-833587d9c1a4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'convert this string to lower case.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}