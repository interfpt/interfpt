{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-nlp/blob/main/NLP%20-%20Session%2016%20-%20Keras%20-%20Glove%20Vectors%20Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebzAitzAy1Va"
      },
      "source": [
        "**NLP - Session 16 - Keras - Glove Vectors Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZ0pL68y1Vd"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "`GloVe` stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating a global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n",
        "#### Ref:\n",
        " - Glove Vectors: \n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        " - Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download):\n",
        "\n",
        "http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "\n",
        " - Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): \n",
        "\n",
        "http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "\n",
        "#### GloVe\n",
        "Glove is one of the text encodings patterns. If you have the NLP project in your hand then Glove or Word2Vec are important topics.\n",
        "\n",
        "### But the first question is What is Glove?\n",
        "\n",
        "##### Glove: Global Vectors Word Representation.\n",
        "We know that a machine can understand only the numbers. The machine will not understand what is mean by `“I am Indian”`. So to transform this into numbers there are some mechanisms. We can use the `One-Hot-Encoding` also. In One-Hot-Encoding we create a matrix of the n-dimension of the words against a vocabulary. Vocabulary is a list of words and then we put 1 in the row where word of our text matches and the rest of the places are 0.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "But there is a problem if you see these are just the word representation in 0 and 1 but there is no linking or direction in the numbers or words Or we can not find the distance between the 2 words or 2 sentences using this encoding method.\n",
        "\n",
        "#### But the second question is what we are going to do by using this distance between the 2 words. \n",
        "Yes, definitely question is important. So just take a simple example, we have one problem statement and we have 10 documents and we have to find the given text is the best matching to which of the 10 documents. Here we can not just use some word matching algorithms and say that these specific words are matching in a few of the document As `It Is because there may be some other document that has similar words and not the exact word as per query text`. So to find the similarity between the words we need a vector that will give us the word representation in different dimensions and then we can compare this word vector with another word vector and find the distance between them.\n",
        "\n",
        "To accomplish this task we need to find the vector representation of the word. And one of the best ways to find word representation in vector-matrix is `GLOVE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBVrDd6By1Ve"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import (\n",
        "    Activation,\n",
        "    Conv1D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    GlobalMaxPooling1D,\n",
        "    MaxPooling1D,\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfY2u98Gy1Vf"
      },
      "outputs": [],
      "source": [
        "# reading dataset\n",
        "\n",
        "df = pd.read_csv(\"data/twitter4000.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnBqFWJMy1Vg"
      },
      "source": [
        "#### Preprocessing and Cleaning\n",
        "Here, we are doing the text processing where we are performing below steps :\n",
        "\n",
        " - Expanding the contracted words or tokens\n",
        " - Removing Email\n",
        " - Removing URLs and HTML tags\n",
        " - Removing ‘RT’ retweet tags\n",
        " - Replacing all non-alphabets values with null\n",
        " - We are defining a dictionary contractions to replace all the short text values with their corresponding expanded values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE1Ikh-7y1Vg"
      },
      "outputs": [],
      "source": [
        "# dictionary `contractions` to replace all the short text values with their corresponding the expanded values\n",
        "# you can add more values as per your requirements.\n",
        "\n",
        "contractions = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how does\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \" u \": \" you \",\n",
        "    \" ur \": \" your \",\n",
        "    \" n \": \" and \",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9zImON5y1Vh"
      },
      "source": [
        "In the below function `get_clean_text()`, we are performing all the data cleaning activities like `expanding the contracted words or tokens, removing Email , removing URLs and HTML tags , removing ‘RT’ retweet tags and replacing all non alphabets values with null`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZMHly3Wy1Vi"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import re\n",
        "\n",
        "text = ' '.join(df['twitts'])\n",
        "text = text.split()\n",
        "freq_comm = pd.Series(text).value_counts()\n",
        "rare = freq_comm[freq_comm.values == 1]\n",
        "\n",
        "def get_clean_text(x):\n",
        "    if type(x) is str:\n",
        "        x = x.lower()\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        x = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x) \n",
        "        #regex to remove to emails\n",
        "        x = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)\n",
        "        #regex to remove URLs\n",
        "        x = re.sub('RT', \"\", x)\n",
        "        #substitute the 'RT' retweet tags with empty spaces\n",
        "        x = re.sub('[^A-Z a-z]+', '', x)\n",
        "        #combining all the text excluding rare words.\n",
        "        x = ' '.join([t for t in x.split() if t not in rare])\n",
        "        return x\n",
        "    else:\n",
        "        return x\n",
        "    \n",
        "df['twitts'] = df['twitts'].apply(lambda x: get_clean_text(x))        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s8r0zdYy1Vi"
      },
      "outputs": [],
      "source": [
        "# Displaying the cleaned texts\n",
        "\n",
        "df[\"twitts\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90yeEphmy1Vl"
      },
      "outputs": [],
      "source": [
        "# Displaying the categorical values\n",
        "\n",
        "df[\"sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6rCO_Lxy1Vm"
      },
      "outputs": [],
      "source": [
        "# Conversion to list and then displaying the list\n",
        "\n",
        "text = df[\"twitts\"].tolist()\n",
        "text[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNh35hKMy1Vn"
      },
      "outputs": [],
      "source": [
        "# Storing the values of sentiment column to variable y\n",
        "\n",
        "y = df[\"sentiment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcGQoFgSy1Vn"
      },
      "outputs": [],
      "source": [
        "# Tokenizer to read all the words present in our corpus\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6vZA4B-y1Vn"
      },
      "outputs": [],
      "source": [
        "# Declaring the vocab_size\n",
        "\n",
        "vocab_size = len(token.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--6JhT8dy1Vo"
      },
      "outputs": [],
      "source": [
        "# Conversion to numerical formats\n",
        "\n",
        "encoded_text = token.texts_to_sequences(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBAksdHBy1Vo"
      },
      "outputs": [],
      "source": [
        "# Printing the values of encoded texts of top 3 rows\n",
        "\n",
        "print(encoded_text[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdofURAYy1Vo"
      },
      "outputs": [],
      "source": [
        "# max_length = 120 means we are considering max 120 words or token only\n",
        "# padding = 'post' means that we padding post the sentence(keeping values 0 if the tokens are not there)\n",
        "\n",
        "max_length = 120\n",
        "X = pad_sequences(encoded_text, maxlen=max_length, padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRUdVgHoy1Vo"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxauZOkCy1Vp"
      },
      "outputs": [],
      "source": [
        "# Printing the dimension of X array\n",
        "\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSjoTumpy1Vp"
      },
      "source": [
        "### `GloVe Vectors`\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "`# you -0.11076 0.30786 -0.5198 0.035138 0.10368 -0.052505...... -0.35471 0.2331 -0.0067546 -0.18892 0.27837 -0.38501 -0.11408 0.28191 -0.30946 -0.21878 -0.059105 0.47604 0.05661`\n",
        "\n",
        "`# our first text is key and rest are there vector representation in glove`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqbNreqEy1Vp"
      },
      "outputs": [],
      "source": [
        "# Displaying the column 'twitts' of dataframe\n",
        "\n",
        "df[\"twitts\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-nBNiNFy1Vp"
      },
      "outputs": [],
      "source": [
        "# Declaring dict to store all the words as keys in the dictionary and their vector representations as values\n",
        "\n",
        "glove_vectors = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQv8dZKPy1Vp"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# file = open('glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "file = open('data/glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #storing the word in the variable\n",
        "    vectors = np.asarray(values[1: ])\n",
        "    #storing the vector representation of the respective word in the dictionary\n",
        "    glove_vectors[word] = vectors\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTCYlMSFy1Vq"
      },
      "outputs": [],
      "source": [
        "# Printing length of glove vectors\n",
        "len(glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B14ojLeby1Vq"
      },
      "outputs": [],
      "source": [
        "keys = glove_vectors.keys()\n",
        "len(keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWs7TeYOy1Vq"
      },
      "source": [
        "We have total `1193514` key values pairs in our dictionary of `glove vectors`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XidHBqf7y1Vq"
      },
      "outputs": [],
      "source": [
        "glove_vectors.get(\"aassrfdfa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBfkd45Ty1Vq"
      },
      "outputs": [],
      "source": [
        "glove_vectors.get(\"you\").shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqTo60ULy1Vq"
      },
      "source": [
        "#### Observation:\n",
        "1. You can see above that misspelled words are not having their vector representation.\n",
        "\n",
        "2. Since we have taken the glove vectors of 200 dimensions, that’s why the word ‘you’ is having 200 values.\n",
        "\n",
        "Now we are creating a matrix for the tokens which we are having in our dataset and then storing their vector representation values in the matrix if it matches with glove_vectors words else print the misspelled words or words which are not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7EJWrz3y1Vr"
      },
      "outputs": [],
      "source": [
        "word_vector_matrix = np.zeros((vocab_size, 200))\n",
        "\n",
        "for word, index in token.word_index.items():\n",
        "    vector = glove_vectors.get(word)\n",
        "    if vector is not None:\n",
        "        word_vector_matrix[index] = vector\n",
        "    else:\n",
        "        print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u-iLhG1y1Vr"
      },
      "outputs": [],
      "source": [
        "word_vector_matrix[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IyNznBRy1Vr"
      },
      "source": [
        "#### Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0RuVM3_y1Vr"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into train and test dataset\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=42, test_size=0.2, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuKxDyEfy1Vr"
      },
      "source": [
        "Now, we are building a model using `Tensorflow.Keras` library below.\n",
        "\n",
        "Below are explanation of each parameters which we are passing:\n",
        "\n",
        " - `vocab_size` : This is the input dimension in which we will take all the tokens present in our dataset.\n",
        "\n",
        " - `vec_size` : This is the size of the vector space in which words will be embedded.\n",
        "\n",
        " - `input_length` : This is the length of input sequences, as you would define for any input layer of a Keras model.\n",
        "\n",
        " - `weights` : Here we are taking pretrained weights of each word.\n",
        "\n",
        " - `trainable` : Here, we do not want to update the learned word weights in this model(since we are using glove vectors here), therefore we will set the trainable attribute for the model to be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8onvuCey1Vr"
      },
      "outputs": [],
      "source": [
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)\n",
        "y_train = np.asarray(y_train)\n",
        "y_test = np.asarray(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggJTKGGqy1Vr"
      },
      "outputs": [],
      "source": [
        "vec_size = 200\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    Embedding(\n",
        "        vocab_size,\n",
        "        vec_size,\n",
        "        input_length=max_length,\n",
        "        weights=[word_vector_matrix],\n",
        "        trainable=False,\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(Conv1D(64, 8, activation=\"relu\"))\n",
        "# here 64 is number of filters and 8 is size of filters\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4doMmvty1Vs"
      },
      "outputs": [],
      "source": [
        "# Here we will clean text using the same method as we have done above\n",
        "# Using same token object we have used here which we have used during training dataset\n",
        "\n",
        "\n",
        "def get_encode(x):\n",
        "    x = get_clean_text(x)\n",
        "    x = token.texts_to_sequences(x)\n",
        "    x = pad_sequences(x, maxlen=max_length, padding=\"post\")\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNls8lRay1Vs"
      },
      "outputs": [],
      "source": [
        "get_encode([\"i hi how are you isn't\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrgYO9_ry1Vs"
      },
      "outputs": [],
      "source": [
        "# Predicting on text\n",
        "\n",
        "model.predict_classes(get_encode([\"thank you for watching\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POvKUwz3y1Vs"
      },
      "source": [
        "### Summary:\n",
        " - Firstly, we have loaded the dataset using pandas.\n",
        " - After loading the dataset, we have cleaned the dataset using a function get_clean_text().\n",
        " - Then using Tokenizer we have tokenized the entire text corpus.\n",
        " - We have used glove vectors to create a dictionary and then converted it to a weight matrix(used the same during model training).\n",
        " - Here we have used loss function as binary_crossentropy and metric as ‘accuracy’"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "NLP - Session 16 - Keras - Glove Vectors Embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}